
No recompute
--------------
Peak memory 5.5 GB. Time to train 67 seconds

With recompute before bug fix
------------------------------
Peak memory 5.5 GB. Time to train 73 seconds

With recompute after bug fix
-----------------------------
Peak memory 819 MB. Time to train 75 seconds



Training with no recompute. 
----------------------------


loss  tf.Tensor(2.302585, shape=(), dtype=float32)
Filename: run_recompute_grad_with_profile.py

Line #    Mem usage    Increment   Line Contents
================================================
    60    290.9 MiB    290.9 MiB   @profile
    61                             def train_no_recompute(n_steps):
    62    290.9 MiB      0.0 MiB       tf.random.set_seed(123)
    63    290.9 MiB      0.0 MiB       img_dim, n_channels, batch_size = 256, 1, 16
    64    304.0 MiB     13.1 MiB       x, y = get_data(img_dim, n_channels, batch_size)
    65    472.0 MiB    168.1 MiB       model = get_big_cnn_model(img_dim, n_channels, num_partitions=3, blocks_per_partition=3)
    66    472.0 MiB      0.0 MiB       optimizer = optimizers.SGD()
    67    472.0 MiB      0.0 MiB       losses = []
    68    472.0 MiB      0.0 MiB       tr_vars = model.trainable_variables
    69    498.4 MiB      0.0 MiB       for _ in range(n_steps):
    70    472.0 MiB      0.0 MiB           with tf.GradientTape() as tape:
    71   5521.7 MiB   5049.6 MiB               logits = model(x)
    72   5521.7 MiB      0.0 MiB               loss  = compute_loss(logits, y)
    73   5521.7 MiB      0.0 MiB               print('loss ', loss)
    74   5521.7 MiB      0.0 MiB               losses.append(loss)       
    75    657.6 MiB      0.0 MiB           grads = tape.gradient(loss, tr_vars) # tr_vars
    76    657.6 MiB      0.0 MiB           optimizer.apply_gradients(zip(grads, tr_vars))
    77    498.4 MiB      0.0 MiB           del grads 
    78    498.4 MiB      0.0 MiB       return losses


Time elapsed is  66.72278451919556  seconds


Training with recompute before recompute_grad bug fix
-------------------------------------------------------

Line #    Mem usage    Increment   Line Contents
================================================
    80    292.4 MiB    292.4 MiB   @profile
    81                             def train_tf_recompute(n_steps):
    82    292.6 MiB      0.2 MiB       tf.random.set_seed(123)
    83    292.6 MiB      0.0 MiB       img_dim, n_channels, batch_size = 256, 1, 16
    84    304.9 MiB     12.3 MiB       x, y = get_data(img_dim, n_channels, batch_size)
    85    473.0 MiB    168.1 MiB       models = get_split_cnn_model(img_dim, n_channels, num_partitions=3, blocks_per_partition=3)
    86    473.0 MiB      0.0 MiB       model1, model2, model3 = models
    87    473.0 MiB      0.0 MiB       model1_re = tf.recompute_grad(model1)
    88    473.0 MiB      0.0 MiB       model2_re = tf.recompute_grad(model2)
    89    473.0 MiB      0.1 MiB       model3_re = tf.recompute_grad(model3)
    90    473.0 MiB      0.0 MiB       optimizer = optimizers.SGD()
    91    473.0 MiB      0.0 MiB       tr_vars = model1.trainable_variables + model2.trainable_variables + model3.trainable_variables
    92    473.0 MiB      0.0 MiB       losses = []
    93    660.1 MiB      0.0 MiB       for _ in range(n_steps):
    94    473.0 MiB      0.0 MiB           with tf.GradientTape() as tape:
    95   2162.5 MiB   1689.5 MiB               logits1 = model1_re(x)
    96   3842.7 MiB   1680.2 MiB               logits2 = model2_re(logits1)
    97   5522.9 MiB   1680.2 MiB               logits3 = model3_re(logits2)
    98   5522.9 MiB      0.0 MiB               loss  = compute_loss(logits3, y)
    99   5522.9 MiB      0.0 MiB               print('loss ', loss)
   100   5522.9 MiB      0.0 MiB               losses.append(loss)
   101    819.6 MiB      0.0 MiB           grads = tape.gradient(loss, tr_vars) # tr_vars
   102    819.6 MiB      0.0 MiB           optimizer.apply_gradients(zip(grads, tr_vars))
   103    660.1 MiB      0.0 MiB           del grads 
   104    660.1 MiB      0.0 MiB       return losses


Time elapsed is  73.41292262077332  seconds


Training with recompute after recompute_grad bug fix
-------------------------------------------------------

Line #    Mem usage    Increment   Line Contents
================================================
    80    291.7 MiB    291.7 MiB   @profile
    81                             def train_tf_recompute(n_steps):
    82    292.0 MiB      0.2 MiB       tf.random.set_seed(123)
    83    292.0 MiB      0.0 MiB       img_dim, n_channels, batch_size = 256, 1, 16
    84    304.4 MiB     12.4 MiB       x, y = get_data(img_dim, n_channels, batch_size)
    85    472.3 MiB    167.9 MiB       models = get_split_cnn_model(img_dim, n_channels, num_partitions=3, blocks_per_partition=3)
    86    472.3 MiB      0.0 MiB       model1, model2, model3 = models
    87    472.3 MiB      0.0 MiB       model1_re = tf.recompute_grad(model1)
    88    472.3 MiB      0.1 MiB       model2_re = tf.recompute_grad(model2)
    89    472.3 MiB      0.0 MiB       model3_re = tf.recompute_grad(model3)
    90    472.3 MiB      0.0 MiB       optimizer = optimizers.SGD()
    91    472.3 MiB      0.0 MiB       tr_vars = model1.trainable_variables + model2.trainable_variables + model3.trainable_variables
    92    472.3 MiB      0.0 MiB       losses = []
    93    659.2 MiB      0.0 MiB       for _ in range(n_steps):
    94    472.3 MiB      0.0 MiB           with tf.GradientTape() as tape:
    95    561.5 MiB     89.2 MiB               logits1 = model1_re(x)
    96    641.6 MiB     80.1 MiB               logits2 = model2_re(logits1)
    97    642.2 MiB      0.5 MiB               logits3 = model3_re(logits2)
    98    642.2 MiB      0.0 MiB               loss  = compute_loss(logits3, y)
    99    642.2 MiB      0.0 MiB               print('loss ', loss)
   100    642.2 MiB      0.0 MiB               losses.append(loss)
   101    818.6 MiB    176.4 MiB           grads = tape.gradient(loss, tr_vars) # tr_vars
   102    818.6 MiB      0.0 MiB           optimizer.apply_gradients(zip(grads, tr_vars))
   103    659.2 MiB      0.0 MiB           del grads 
   104    659.2 MiB      0.0 MiB       return losses


Time elapsed is  74.54963183403015  seconds
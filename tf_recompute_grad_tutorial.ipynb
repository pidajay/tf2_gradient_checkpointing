{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is gradient checkpointing?\n",
    "Gradient checkpointing enables users to train large models with relatively small memory resources. Large models could refer to\n",
    "1. Models with large variables i.e weight matrices. As a consequence such models have correspondingly large gradients and optimizer states. The activations (intermediate outputs from the model layers) tend to be relatively small (depends on the batch size). Typically fully connected networks and RNNs fall under this category.\n",
    "2. Models with small weights but large activations. CNNs and transformers tend to fall under this category.\n",
    "It is important to note that gradient checkpointing is meant to help with models of type 2. Models of type 1 do not stand to gain much benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "#from tensorflow.python.ops import custom_gradient\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition\n",
    "To make use of tf recompute grad, one has to manually split their 'large' model into manageable 'partitions' (each partition corresponds to a logical 'checkpoint'). Below is an example of a simple 'large' CNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_big_cnn_model(img_dim, n_channels, num_partitions, blocks_per_partition):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(img_dim, img_dim, n_channels)))\n",
    "    for _ in range(num_partitions):\n",
    "        for _ in range(blocks_per_partition):\n",
    "            model.add(layers.Conv2D(10, 5, padding='same', activation=tf.nn.relu))\n",
    "            model.add(layers.MaxPooling2D((1, 1), padding='same'))\n",
    "            model.add(layers.Conv2D(40, 5, padding='same', activation=tf.nn.relu))\n",
    "            model.add(layers.MaxPooling2D((1, 1), padding='same'))\n",
    "            model.add(layers.Conv2D(20, 5, padding='same', activation=tf.nn.relu))\n",
    "            model.add(layers.MaxPooling2D((1, 1), padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation=tf.nn.relu))\n",
    "    model.add(layers.Dense(10))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the large CNN model that has been split into 3 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_cnn_model(img_dim, n_channels, num_partitions, blocks_per_partition):\n",
    "    models = [tf.keras.Sequential() for _ in range(num_partitions)]\n",
    "    models[0].add(layers.Input(shape=(img_dim, img_dim, n_channels)))\n",
    "    for i in range(num_partitions):\n",
    "        model = models[i]\n",
    "        if i > 0:\n",
    "            last_shape = models[i-1].layers[-1].output_shape\n",
    "            model.add(layers.Input(shape=last_shape[1:]))\n",
    "        for _ in range(blocks_per_partition):\n",
    "            model.add(layers.Conv2D(10, 5, padding='same', activation=tf.nn.relu))\n",
    "            model.add(layers.MaxPooling2D((1, 1), padding='same'))\n",
    "            model.add(layers.Conv2D(40, 5, padding='same', activation=tf.nn.relu))\n",
    "            model.add(layers.MaxPooling2D((1, 1), padding='same'))\n",
    "            model.add(layers.Conv2D(20, 5, padding='same', activation=tf.nn.relu))\n",
    "            model.add(layers.MaxPooling2D((1, 1), padding='same'))\n",
    "    models[-1].add(layers.Flatten())\n",
    "    models[-1].add(layers.Dense(32, activation=tf.nn.relu))\n",
    "    models[-1].add(layers.Dense(10))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels):\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(img_dim, n_channels, batch_size):\n",
    "    inputs = tf.ones([batch_size,img_dim,img_dim,n_channels])\n",
    "    labels = tf.ones([batch_size], dtype=tf.int64)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This training loop should produce an OOM exception on a GPU with 16GB memory\n",
    "def train(n_steps):\n",
    "    tf.random.set_seed(123)\n",
    "    img_dim, n_channels, batch_size = 256, 1, 16\n",
    "    x, y = get_data(img_dim, n_channels, batch_size)\n",
    "    model = get_big_cnn_model(img_dim, n_channels, num_partitions=3, blocks_per_partition=9)\n",
    "    optimizer = optimizers.SGD()\n",
    "    losses = []\n",
    "    tr_vars = model.trainable_variables\n",
    "    for _ in range(n_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss  = compute_loss(logits, y)\n",
    "            print('loss ', loss)\n",
    "            losses.append(loss)       \n",
    "        grads = tape.gradient(loss, tr_vars) # tr_vars\n",
    "        optimizer.apply_gradients(zip(grads, tr_vars))\n",
    "        del grads \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This training loop should be able to run successfully. Infact you can more double the model size by setting blocks_per_partition = 20\n",
    "# and still train successfully\n",
    "def train_tf_recompute_split(n_steps):\n",
    "    tf.random.set_seed(123)\n",
    "    img_dim, n_channels, batch_size = 256, 1, 16\n",
    "    x, y = get_data(img_dim, n_channels, batch_size)\n",
    "    models = get_split_cnn_model(img_dim, n_channels, num_partitions=3, blocks_per_partition=9)\n",
    "    model1, model2, model3 = models\n",
    "    model1_re = tf.recompute_grad(model1)\n",
    "    model2_re = tf.recompute_grad(model2)\n",
    "    model3_re = tf.recompute_grad(model3)\n",
    "    optimizer = optimizers.SGD()\n",
    "    tr_vars = model1.trainable_variables + model2.trainable_variables + model3.trainable_variables\n",
    "    losses = []\n",
    "    for _ in range(n_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits1 = model1_re(x)\n",
    "            logits2 = model2_re(logits1)\n",
    "            logits3 = model3_re(logits2)\n",
    "            loss  = compute_loss(logits3, y)\n",
    "            print('loss ', loss)\n",
    "            losses.append(loss)\n",
    "        grads = tape.gradient(loss, tr_vars) # tr_vars\n",
    "        optimizer.apply_gradients(zip(grads, tr_vars))\n",
    "        del grads \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_tf_recompute_split(1)\n",
    "end = time.time()\n",
    "print('Time elapsed is ', end - start, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_part = 3 and blocks_per_part = 7\n",
    "# recomp\n",
    "# losstf.Tensor(2.3025851, shape=(), dtype=float32)\n",
    "# losstf.Tensor(2.292471, shape=(), dtype=float32)\n",
    "# losstf.Tensor(2.2751424, shape=(), dtype=float32)\n",
    "\n",
    "# no recomp\n",
    "# losstf.Tensor(2.3025851, shape=(), dtype=float32)\n",
    "# losstf.Tensor(2.2924523, shape=(), dtype=float32)\n",
    "# losstf.Tensor(2.2754436, shape=(), dtype=float32)\n",
    "# Time elapsed is  17.34099841117859  seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bitc6497a0aa9a94ae0a3cd35c91c6c1677"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
